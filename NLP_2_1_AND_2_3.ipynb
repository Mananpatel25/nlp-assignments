{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMfvoXrRyOvjgrlqQ8ykr4E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mananpatel25/nlp-assignments/blob/main/NLP_2_1_AND_2_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxiN40na_sqF",
        "outputId": "68080f72-82bf-4e62-a4df-55032e7f9e2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.11/dist-packages (3.2)\n",
            "Loading embedding models...\n",
            "Downloading embedding models if not present...\n",
            "Downloading LexVec...\n",
            "\n",
            "Evaluating standard analogies...\n",
            "\n",
            "Results for LexVec:\n",
            "capital-world: 0.00% (0/4524)\n",
            "currency: 0.00% (0/866)\n",
            "city-in-state: 0.00% (0/2467)\n",
            "family: 87.75% (444/506)\n",
            "gram1-adjective-to-adverb: 24.90% (247/992)\n",
            "gram2-opposite: 36.58% (297/812)\n",
            "gram3-comparative: 87.31% (1163/1332)\n",
            "gram6-nationality-adjective: 0.00% (0/1599)\n",
            "Overall accuracy: 16.42% (2151/13098)\n",
            "\n",
            "Results for FastText CC reduced:\n",
            "capital-world: 95.03% (4299/4524)\n",
            "currency: 15.94% (138/866)\n",
            "city-in-state: 82.53% (2036/2467)\n",
            "family: 91.11% (461/506)\n",
            "gram1-adjective-to-adverb: 34.07% (338/992)\n",
            "gram2-opposite: 47.17% (383/812)\n",
            "gram3-comparative: 92.87% (1237/1332)\n",
            "gram6-nationality-adjective: 91.31% (1460/1599)\n",
            "Overall accuracy: 79.03% (10352/13098)\n",
            "\n",
            "Evaluating custom analogies...\n",
            "\n",
            "Custom Analogy Results for LexVec:\n",
            "happy:smile :: angry:frown -> Predicted: smirk (Correct: False)\n",
            "joy:laugh :: sorrow:cry -> Predicted: chuckle (Correct: False)\n",
            "love:hug :: hate:fight -> Predicted: hugs (Correct: False)\n",
            "cloud:rain :: volcano:lava -> Predicted: rains (Correct: False)\n",
            "wind:blow :: water:flow -> Predicted: setback (Correct: False)\n",
            "seed:plant :: egg:bird -> Predicted: eggs (Correct: False)\n",
            "Custom analogy accuracy: 0.00% (0/6)\n",
            "\n",
            "Custom Analogy Results for FastText CC reduced:\n",
            "happy:smile :: angry:frown -> Predicted: scowl (Correct: False)\n",
            "joy:laugh :: sorrow:cry -> Predicted: chuckle (Correct: False)\n",
            "love:hug :: hate:fight -> Predicted: hugged (Correct: False)\n",
            "cloud:rain :: volcano:lava -> Predicted: volcanoes (Correct: False)\n",
            "wind:blow :: water:flow -> Predicted: water. (Correct: False)\n",
            "seed:plant :: egg:bird -> Predicted: eggs (Correct: False)\n",
            "Custom analogy accuracy: 0.00% (0/6)\n"
          ]
        }
      ],
      "source": [
        "!pip install wget\n",
        "from gensim.models import KeyedVectors\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "from collections import defaultdict\n",
        "import os\n",
        "import gzip\n",
        "import wget\n",
        "\n",
        "def download_embeddings():\n",
        "    print(\"Downloading embedding models if not present...\")\n",
        "\n",
        "\n",
        "    lexvec_url = \"https://www.dropbox.com/s/kguufyc2xcdi8yk/lexvec.enwiki%2Bnewscrawl.300d.W.pos.vectors.gz?dl=1\"\n",
        "    if not os.path.exists('lexvec.txt'):\n",
        "        if not os.path.exists('lexvec.txt.gz'):\n",
        "            print(\"Downloading LexVec...\")\n",
        "            wget.download(lexvec_url, 'lexvec.txt.gz')\n",
        "        with gzip.open('lexvec.txt.gz', 'rb') as f_in:\n",
        "            with open('lexvec.txt', 'wb') as f_out:\n",
        "                f_out.write(f_in.read())\n",
        "\n",
        "    # FastText reduced\n",
        "    fasttext_url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\"\n",
        "    if not os.path.exists('fasttext-cc-reduced.vec'):\n",
        "        if not os.path.exists('cc.en.300.vec.gz'):\n",
        "            print(\"Downloading FastText CC reduced...\")\n",
        "            wget.download(fasttext_url, 'cc.en.300.vec.gz')\n",
        "        # We'll load only the first 200k vectors to keep it lightweight\n",
        "        with gzip.open('cc.en.300.vec.gz', 'rt', encoding='utf-8') as f_in:\n",
        "            with open('fasttext-cc-reduced.vec', 'w', encoding='utf-8') as f_out:\n",
        "                header = next(f_in)\n",
        "                vocab_size, dim = map(int, header.split())\n",
        "                f_out.write(f\"200000 {dim}\\n\")  # Reduced vocabulary size\n",
        "                for i, line in enumerate(f_in):\n",
        "                    if i >= 200000:\n",
        "                        break\n",
        "                    f_out.write(line)\n",
        "\n",
        "def load_embeddings():\n",
        "    \"\"\"Load the embedding models\"\"\"\n",
        "    print(\"Loading embedding models...\")\n",
        "\n",
        "    # Download if not present\n",
        "    download_embeddings()\n",
        "\n",
        "    lexvec = KeyedVectors.load_word2vec_format('lexvec.txt', binary=False)\n",
        "\n",
        "    # Load reduced FastText\n",
        "    fasttext = KeyedVectors.load_word2vec_format('fasttext-cc-reduced.vec', binary=False)\n",
        "\n",
        "    return lexvec, fasttext\n",
        "\n",
        "def cosine_similarity(v1, v2):\n",
        "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
        "    return 1 - cosine(v1, v2)\n",
        "\n",
        "def solve_analogy(a, b, c, vocab, embeddings):\n",
        "    \"\"\"\n",
        "    Solve analogy a:b :: c:x using the method from Mikolov et al. 2013\n",
        "    Returns the most similar word x and its similarity score\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get embeddings for words\n",
        "        a_vec = embeddings[a]\n",
        "        b_vec = embeddings[b]\n",
        "        c_vec = embeddings[c]\n",
        "\n",
        "        # Calculate target vector\n",
        "        target = b_vec - a_vec + c_vec\n",
        "\n",
        "        # Find most similar word (excluding a, b, c)\n",
        "        best_word = None\n",
        "\n",
        "        # Use gensim's optimized most_similar method\n",
        "        results = embeddings.most_similar(positive=[b, c], negative=[a], topn=5)\n",
        "        for word, score in results:\n",
        "            if word not in [a, b, c]:\n",
        "                best_word = word\n",
        "                break\n",
        "\n",
        "        return best_word, score\n",
        "    except KeyError:\n",
        "        return None, 0.0\n",
        "\n",
        "def evaluate_analogies(filename, embeddings, embedding_name):\n",
        "    \"\"\"\n",
        "    Evaluate embedding model on analogy dataset\n",
        "    Returns accuracy per category and overall accuracy\n",
        "    \"\"\"\n",
        "    results = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
        "    current_category = None\n",
        "\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.startswith(':'):\n",
        "                current_category = line.strip()[2:]  # Remove ': ' prefix\n",
        "                continue\n",
        "\n",
        "            if current_category not in [\n",
        "                'capital-world', 'currency', 'city-in-state', 'family',\n",
        "                'gram1-adjective-to-adverb', 'gram2-opposite',\n",
        "                'gram3-comparative', 'gram6-nationality-adjective'\n",
        "            ]:\n",
        "                continue\n",
        "\n",
        "            words = line.strip().split()\n",
        "            if len(words) != 4:\n",
        "                continue\n",
        "\n",
        "            a, b, c, expected = words\n",
        "            predicted, _ = solve_analogy(a, b, c, embeddings.key_to_index, embeddings)\n",
        "\n",
        "            if predicted == expected:\n",
        "                results[current_category]['correct'] += 1\n",
        "            results[current_category]['total'] += 1\n",
        "\n",
        "    # Calculate accuracies\n",
        "    print(f\"\\nResults for {embedding_name}:\")\n",
        "    total_correct = 0\n",
        "    total_questions = 0\n",
        "\n",
        "    for category in results:\n",
        "        correct = results[category]['correct']\n",
        "        total = results[category]['total']\n",
        "        accuracy = (correct / total * 100) if total > 0 else 0\n",
        "        print(f\"{category}: {accuracy:.2f}% ({correct}/{total})\")\n",
        "        total_correct += correct\n",
        "        total_questions += total\n",
        "\n",
        "    overall_accuracy = (total_correct / total_questions * 100) if total_questions > 0 else 0\n",
        "    print(f\"Overall accuracy: {overall_accuracy:.2f}% ({total_correct}/{total_questions})\")\n",
        "    return results\n",
        "\n",
        "def custom_analogy_test(embeddings, embedding_name):\n",
        "    \"\"\"\n",
        "    Test custom analogy questions with carefully chosen relations\n",
        "    \"\"\"\n",
        "    custom_analogies = [\n",
        "        # Type 1: Emotional States (testing conceptual understanding)\n",
        "        ['happy', 'smile', 'angry', 'frown'],\n",
        "        ['joy', 'laugh', 'sorrow', 'cry'],\n",
        "        ['love', 'hug', 'hate', 'fight'],\n",
        "\n",
        "        # Type 2: Natural Phenomena (testing scientific relationships)\n",
        "        ['cloud', 'rain', 'volcano', 'lava'],\n",
        "        ['wind', 'blow', 'water', 'flow'],\n",
        "        ['seed', 'plant', 'egg', 'bird']\n",
        "    ]\n",
        "\n",
        "    print(f\"\\nCustom Analogy Results for {embedding_name}:\")\n",
        "    correct = 0\n",
        "    total = len(custom_analogies)\n",
        "\n",
        "    for analogy in custom_analogies:\n",
        "        a, b, c, expected = analogy\n",
        "        predicted, score = solve_analogy(a, b, c, embeddings.key_to_index, embeddings)\n",
        "        is_correct = predicted == expected\n",
        "        if is_correct:\n",
        "            correct += 1\n",
        "        print(f\"{a}:{b} :: {c}:{expected} -> Predicted: {predicted} (Correct: {is_correct})\")\n",
        "\n",
        "    accuracy = (correct / total * 100)\n",
        "    print(f\"Custom analogy accuracy: {accuracy:.2f}% ({correct}/{total})\")\n",
        "    return accuracy\n",
        "\n",
        "def main():\n",
        "    # Load embeddings\n",
        "    lexvec, fasttext = load_embeddings()\n",
        "\n",
        "    # Evaluate on standard analogies\n",
        "    print(\"\\nEvaluating standard analogies...\")\n",
        "    lexvec_results = evaluate_analogies('word-test.v1.txt', lexvec, 'LexVec')\n",
        "    fasttext_results = evaluate_analogies('word-test.v1.txt', fasttext, 'FastText CC reduced')\n",
        "\n",
        "    # Evaluate on custom analogies\n",
        "    print(\"\\nEvaluating custom analogies...\")\n",
        "    lexvec_custom = custom_analogy_test(lexvec, 'LexVec')\n",
        "    fasttext_custom = custom_analogy_test(fasttext, 'FastText CC reduced')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nr-kCMl6BPL3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}